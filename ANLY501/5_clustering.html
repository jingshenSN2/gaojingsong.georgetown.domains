<!DOCTYPE HTML>
<html>
<head>
    <title>Jingsong Gao - 501 Portfolio</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="style.css"/>
    <link rel="shortcut icon" href="/icon.png"/>
    <link rel="bookmark" href="/icon.png"/>
</head>

<body>
<div class="headerpage-501"></div>
<div class="main">
    <div class="left-side">
        <div class="left-navi">
            <ul>
                <li class="nav"><p>PC Component</p></li>
                <li class="nav"><a href="#pc-pre">Preparation</a></li>
                <li class="nav"><a href="#pc-uneq">Unequal-Len Time Series</a></li>
                <li class="nav"><a href="#pc-eq">Equal-Len Time Series</a></li>
                <li class="nav"><a href="#pc-sum">Summary</a></li>
                <li class="nav"><p>Crypto & Commodity</p></li>
                <li class="nav"><a href="#cc-model">Clustering Models</a></li>
                <li class="nav"><a href="#cc-pred">Prediction</a></li>
                <li class="nav"><a href="#cc-sum">Summary</a></li>
                <li class="nav"><p>YouTube Comments</p></li>
                <li class="nav"><a href="#youtube-pre">Preparation</a></li>
                <li class="nav"><a href="#youtube-km">KMeans Clustering</a></li>
                <li class="nav"><a href="#youtube-other">DBScan & Hierarchical</a></li>
                <li class="nav"><a href="#youtube-sum">Summary</a></li>
            </ul>
        </div>
    </div>

    <div class="content">
        <h1 style="text-align:center">Clustering</h1>

        <h2>1. PC Component Price Time Series (Python Record Data)</h2>

        <h3>1.1 Pre-classification of PC components</h3>

        <section id="pc-pre">

            <p>The python script for this part is <a
                    href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/blob/master/data_clean/amazon/timeseries_converter.ipynb">data_clean/amazon/timeseries_converter.ipynb</a>.
                All the preprocessed time series can be found at <a
                        href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/tree/master/data/amazon">data/amazon</a>
                folder on GitHub (see below for the definition of each file).</p>

            <p>Data in amazon database is recorded in <b>unequal-interval</b> time series, since Keepa API write a new
                value to their database <b>only</b> when the price changes.</p>

            <p>So, the first step of preprocessing is to rebuild a <b>equal-interval</b> time series from the database.
                <b>Pandas</b> is a useful tool to do this. Since it not only provides an SQL interface but also provides
                a powerful function called <b>resample</b>. With Pandas, historical price data in SQLite database can
                be easily converted into equal-interval time series. <b>After several attempts, the time interval for
                    resampling is assigned to 1 day</b>.</p>

            <table width="100%">
                <caption>File prefix in pre-classified data and their meaning</caption>
                <tr>
                    <th>File Prefix</th>
                    <th>Meaning</th>
                </tr>
                <tr>
                    <td>NEWS</td>
                    <td>price history of the New in marketplace</td>
                </tr>
                <tr>
                    <td>USED</td>
                    <td>price history of the Used in marketplace</td>
                </tr>
                <tr>
                    <td>BUY_BOX</td>
                    <td>price history of the New buy box</td>
                </tr>
                <tr>
                    <td>EBAY_NEW</td>
                    <td>price history of the lowest new price on the respective eBay locale</td>
                </tr>
                <tr>
                    <td>EBAY_USED</td>
                    <td>price history of the lowest used price on the respective eBay locale</td>
                </tr>
            </table>

            <p>Another thing needs to be considered is that Keepa API uses <b>-1</b> as out-of-stock, but these negative
                values will disturb the following clustering. Therefore, all <b>-1</b>s in the time series are filled
                with a previous non-negative value to create a <b>normal price time series</b>.</p>

            <p>But the out-of-stock data does contain some useful information, so a new time series called <b>out-of-stock
                indicator</b> that records the out-of-stock status is created, which all normal prices are converted
                into 0 and out-of-stock values are converted into 1. </p>

            <p>Since the products are released in varied date, some products have data more than 4 years, but some
                products only have data less than 6 months. A pre-classification is performed on the products, all data
                are separated into different files based on their time span. The files are labeled by some suffixes.</p>

            <table width="100%">
                <caption>File suffix in pre-classified data and their meaning</caption>
                <tr>
                    <th>File Suffix</th>
                    <th>Meaning</th>
                </tr>
                <tr>
                    <td>s</td>
                    <td>time span less than 6 months</td>
                </tr>
                <tr>
                    <td>6m</td>
                    <td>time span between 6 months to 1 year</td>
                </tr>
                <tr>
                    <td>1y</td>
                    <td>time span between 1 year to 2 years</td>
                </tr>
                <tr>
                    <td>2y</td>
                    <td>time span between 2 years to 4 years</td>
                </tr>
                <tr>
                    <td>4y</td>
                    <td>time span more than 4 years</td>
                </tr>
                <tr>
                    <td>ts</td>
                    <td>time series, each row contains a time series of normal price separate by space.</td>
                </tr>
                <tr>
                    <td>osts</td>
                    <td>time series, each row contains a time series of out-of-stock indicators separate by space.</td>
                </tr>
                <tr>
                    <td>eqts</td>
                    <td>time series of normal prices but are truncated to same length.</td>
                </tr>
                <tr>
                    <td>eqosts</td>
                    <td>time series of out-of-stock indicators but are truncated to same length.</td>
                </tr>
                <tr>
                    <td>label</td>
                    <td>label, each row contains product type + product name as label</td>
                </tr>
            </table>

            <p>Because some distance metrics in the following clustering require an equal-length time series, so time
                series all have a truncated version. For example, <a
                        href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/blob/master/data/amazon/BUY_BOX_eqts_1y.txt">BUY_BOX_eqts_1y.txt</a>
                contains the same products as <a
                        href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/blob/master/data/amazon/BUY_BOX_ts_1y.txt">BUY_BOX_ts_1y.txt</a>,
                but the time spans of the data are truncated to exactly 1 year.</p>

        </section>

        <h3>1.2 Clustering on unequal-length Time Series (KMeans, metric: DTW, Soft-DTW)</h3>

        <section id="pc-uneq">
            <p>The python script for this part is <a
                    href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/blob/master/cluster/amazon.ipynb">cluster/amazon.ipynb</a>.
                The pre-classified datasets are unequal-length time series. For example, the length of time series
                in
                <a href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/blob/master/data/amazon/BUY_BOX_ts_1y.txt">BUY_BOX_ts_1y.txt</a>
                can vary from 360(1 year) to 719(2 year).</p>

            <p>DTW(<a href="https://tslearn.readthedocs.io/en/stable/user_guide/dtw.html">Dynamic Time Warping</a>) is a
                better
                metric to measure the similarity between time series compare to euclidean metric, especially for
                unequal-length time series(which euclidean metric is no longer available). The <b>tslearn</b> package
                provides
                this metric in Python and a time series version of KMeans clustering algorithm. A TimeSeriesKMeans model
                using DTW metric with n_clusters=8 is trained by the time series in <a
                        href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/blob/master/data/amazon/NEW_ts_1y.txt">NEW_ts_1y.txt</a>.
                The table of clustering
                results shows below. </p>

            <table style="text-align:center;width:100%">
                <caption>Type of products in each cluster of TimeSeriesKMeans model using DTW metric</caption>
                <col width=25%>
                <col width=25%>
                <col width=25%>
                <col width=25%>
                <tr>
                    <th style="text-align:center"></th>
                    <th style="text-align:center">CPU</th>
                    <th style="text-align:center">GPU</th>
                    <th style="text-align:center">Drive</th>
                </tr>
                <tr>
                    <td>Cluster 1</td>
                    <td>2</td>
                    <td><b>16</b></td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>Cluster 2</td>
                    <td>5</td>
                    <td></td>
                    <td><b>20</b></td>
                </tr>
                <tr>
                    <td>Cluster 3</td>
                    <td><b>10</b></td>
                    <td>1</td>
                    <td><b>28</b></td>
                </tr>
                <tr>
                    <td>Cluster 4</td>
                    <td>1</td>
                    <td></td>
                    <td><b>6</b></td>
                </tr>
                <tr>
                    <td>Cluster 5</td>
                    <td></td>
                    <td><b>27</b></td>
                    <td></td>
                </tr>
                <tr>
                    <td>Cluster 6</td>
                    <td>4</td>
                    <td><b>14</b></td>
                    <td></td>
                </tr>
                <tr>
                    <td>Cluster 7</td>
                    <td>3</td>
                    <td>2</td>
                    <td><b>12</b></td>
                </tr>
                <tr>
                    <td>Cluster 8</td>
                    <td>6</td>
                    <td>4</td>
                    <td><b>28</b></td>
                </tr>
            </table>

            <p>The visualization of clusters shows below.</p>

            <img style="width:95%" class="centerfig" src="picture/cluster/new_1y_dtw_8.png">
            <figurecaption>Figure: visualization of clusters in TimeSeriesKMeans model using DTW metric</figurecaption>

            <p>Some patterns can be found in the clustering table and visualization: </p>
            <p>(1) Cluster 1,5,6 are both GPU-dominated clusters, they all show increasing trends in price which
                indicates an abnormal impact on their prices.</p>
            <p>(2) Cluster 2,3,7,8 are both drive-dominated clusters, they all follow a normal decreasing trend in price
                which occurs in most goods.</p>
            <p>(3) There's no CPU-dominated cluster, but most CPUs(25) follow the normal decreasing trend and only 6
                CPUs follow the abnormal increasing trend.</p>

            <p>The cluster centers of DTW metric are kind of sharp that have many sudden jumps. A variant of DTW names
                <a href="https://tslearn.readthedocs.io/en/stable/user_guide/dtw.html#soft-dtw">soft-DTW</a> gives much
                smoother cluster centers. A TimeSeriesKMeans model using soft-DTW metric with n_clusters=6 is trained by
                the same time series. The table of cluster result shows below.</p>

            <table style="text-align:center;width:100%">
                <caption>Type of products in each cluster of TimeSeriesKMeans model using soft-DTW metric</caption>
                <col width=25%>
                <col width=25%>
                <col width=25%>
                <col width=25%>
                <tr>
                    <th style="text-align:center"></th>
                    <th style="text-align:center">CPU</th>
                    <th style="text-align:center">GPU</th>
                    <th style="text-align:center">Drive</th>
                </tr>
                <tr>
                    <td>Cluster 1</td>
                    <td>1</td>
                    <td><b>26</b></td>
                    <td></td>
                </tr>
                <tr>
                    <td>Cluster 2</td>
                    <td><b>15</b></td>
                    <td>3</td>
                    <td><b>40</b></td>
                </tr>
                <tr>
                    <td>Cluster 3</td>
                    <td>8</td>
                    <td>4</td>
                    <td><b>38</b></td>
                </tr>
                <tr>
                    <td>Cluster 4</td>
                    <td>1</td>
                    <td><b>17</b></td>
                    <td></td>
                </tr>
                <tr>
                    <td>Cluster 5</td>
                    <td>5</td>
                    <td></td>
                    <td><b>16</b></td>
                </tr>
                <tr>
                    <td>Cluster 6</td>
                    <td>1</td>
                    <td><b>14</b></td>
                    <td></td>
                </tr>
            </table>

            <p>The visualization of clusters shows below.</p>

            <img style="width:95%" class="centerfig" src="picture/cluster/new_1y_softdtw_6.png">
            <figurecaption>Figure: visualization of clusters in TimeSeriesKMeans model using soft-DTW metric
            </figurecaption>

            <p>Similar patterns can be found in this model: </p>
            <p>(1) Cluster 1,4,6 are both GPU-dominated clusters, they all show increasing trends in price which
                indicates an abnormal impact on their prices.</p>
            <p>(2) Cluster 2,3,5 are both drive-dominated clusters, they all follow a normal decreasing trend in price
                which occurs in most goods.</p>
            <p>(3) There's no CPU-dominated cluster, but most CPUs(28) follow the normal decreasing trend and only 3
                CPUs follow the abnormal increasing trend.</p>
            <p>(4) The cluster centers are much smoother that the trends are clearly shown from them.</p>

        </section>


        <h3>1.3 Clustering on equal-length Time Series (KMeans, metric: Euclidean, Cross-correlation)</h3>

        <section id="pc-eq">

            <p>The python script for this part is <a
                    href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/blob/master/cluster/amazon.ipynb">cluster/amazon.ipynb</a>.
            </p>

            <p>Some metrics like euclidean and cross correlation in tslearn need equal-length time series as input.
                First, a TimeSeriesKMeans model using euclidean metric with n_clusters=6 is trained by the time series
                in
                <a href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/blob/master/data/amazon/NEW_eqts_1y.txt">NEW_eqts_1y.txt</a>.
                The table of cluster result shows below.</p>

            <table style="text-align:center;width:100%">
                <caption>Type of products in each cluster of TimeSeriesKMeans model using euclidean metric</caption>
                <col width=25%>
                <col width=25%>
                <col width=25%>
                <col width=25%>
                <tr>
                    <th style="text-align:center"></th>
                    <th style="text-align:center">CPU</th>
                    <th style="text-align:center">GPU</th>
                    <th style="text-align:center">Drive</th>
                </tr>
                <tr>
                    <td>Cluster 1</td>
                    <td>1</td>
                    <td><b>14</b></td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>Cluster 2</td>
                    <td><b>14</b></td>
                    <td>2</td>
                    <td><b>44</b></td>
                </tr>
                <tr>
                    <td>Cluster 3</td>
                    <td>8</td>
                    <td>4</td>
                    <td><b>38</b></td>
                </tr>
                <tr>
                    <td>Cluster 4</td>
                    <td>1</td>
                    <td><b>26</b></td>
                    <td>2</td>
                </tr>
                <tr>
                    <td>Cluster 5</td>
                    <td>1</td>
                    <td>1</td>
                    <td><b>19</b></td>
                </tr>
                <tr>
                    <td>Cluster 6</td>
                    <td>1</td>
                    <td><b>18</b></td>
                    <td></td>
                </tr>
            </table>

            <p>The visualization of clusters shows below.</p>

            <img style="width:95%" class="centerfig" src="picture/cluster/new_1y_euclidean_6.png">
            <figurecaption>Figure: visualization of clusters in TimeSeriesKMeans model using euclidean metric
            </figurecaption>

            <p>This model gives a very similar clustering result with the DTW model and soft-DTW model. But the
                smoothness of cluster centers are between two DTW models.</p>

            <p>Then, a KShape model using cross-correlation metric with n_clusters=4 is trained by the same time series.
                The table of cluster result shows below.</p>

            <table style="text-align:center;width:100%">
                <caption>Type of products in each cluster of KShape model using cross-correlation metric</caption>
                <col width=25%>
                <col width=25%>
                <col width=25%>
                <col width=25%>
                <tr>
                    <th style="text-align:center"></th>
                    <th style="text-align:center">CPU</th>
                    <th style="text-align:center">GPU</th>
                    <th style="text-align:center">Drive</th>
                </tr>
                <tr>
                    <td>Cluster 1</td>
                    <td><b>17</b></td>
                    <td></td>
                    <td><b>16</b></td>
                </tr>
                <tr>
                    <td>Cluster 2</td>
                    <td></td>
                    <td><b>17</b></td>
                    <td>1</td>
                </tr>
                <tr>
                    <td>Cluster 3</td>
                    <td>14</td>
                    <td>12</td>
                    <td><b>76</b></td>
                </tr>
                <tr>
                    <td>Cluster 4</td>
                    <td></td>
                    <td><b>35</b></td>
                    <td>1</td>
                </tr>
            </table>

            <p>The visualization of clusters shows below.</p>

            <img style="width:95%" class="centerfig" src="picture/cluster/new_1y_kshape_4.png">
            <figurecaption>Figure: visualization of clusters in KShape model using cross-correlation metric
            </figurecaption>

            <p>The number of clusters of this model is only 4, 2 GPU-dominated clusters and 2 CPU-drive-mixed
                clusters.</p>

        </section>

        <h3>1.4 Summary of Time Series Clustering by Python</h3>

        <section id="pc-sum">

            <p>In this part, KMeans clustering is applied to the one-year Amazon historical price data with four
                different distance metrics including DTW, soft-DTW, euclidean, and cross-correlation. All models give
                similar clustering results: graphic cards are in clusters which follow an abnormal increasing trend of
                prices, CPUs and drives are in clusters which follow a normal decreasing trend of prices. This result
                suggests that graphic cards have taken the biggest impact from the pandemic and cryptocurrency boom.</p>

            <p>The DTW and soft-DTW metrics show a great performance on clustering the unequal-length time series. And
                the soft-DTW and euclidean metrics give the most clear clustering centers. However, the implementations
                in tslearn package are slow, especially the soft-DTW metric. Therefore, the k values in these models are
                chosen arbitrarily without any validation like elbow rule or silhouette coefficient.</p>

        </section>

        <h2>2. Crypto & Commodity (R Record Data)</h2>

        <p>The R script for this part is <a
                href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/blob/master/cluster/crypto.rmd">cluster/crypto.rmd</a>.
        </p>

        <section id="cc-model">
            <h3>2.1 Preparation of Crypto & Commodity Time Series</h3>

            <p>Time series of 8 cryptocurrencies(BTC,ETH,DOGE,LTC,BCH,ETC,XMR,DASH) and 5
                commodities(Aluminum,Copper,Gold,Silver,Crude Oil) are cleaned from the raw data from Yahoo Finance and
                Coin Metrics. To be consistent with the following clustering model, all time series are truncated to
                recent 2 years(720 days) and their prices are re-scaled to (0,1) interval. </p>

            <h3>2.2 Clustering Model based on Amazon Prices</h3>

            <p>Since there are only 13 records of crypto & commodities, it's better to cluster them based on an existed
                clustering model. In this part, an R package <b>dtwclust</b> is used to build partition and hierarchical
                models. All models are trained by the time series in <a
                        href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/blob/master/data/amazon/NEW_eqts_2y.txt">NEW_eqts_2y.txt</a>.
            </p>

            <p>First, KMeans models with k = 4:10 are trained and evaluated by multiple validation parameters. The
                result is shown below.</p>


            <table style="text-align:center;width:100%">
                <caption>Validation parameters of KMeans models with k = 4:10</caption>
                <thead>
                <tr>
                    <th></th>
                    <th style="text-align:center">k_4</th>
                    <th style="text-align:center">k_5</th>
                    <th style="text-align:center">k_6</th>
                    <th style="text-align:center">k_7</th>
                    <th style="text-align:center">k_8</th>
                    <th style="text-align:center">k_9</th>
                    <th style="text-align:center">k_10</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>Sil</td>
                    <td>0.278</td>
                    <td><b>0.311</b></td>
                    <td>0.232</td>
                    <td>0.189</td>
                    <td>0.194</td>
                    <td>0.154</td>
                    <td>0.130</td>
                </tr>
                <tr>
                    <td>CH</td>
                    <td><b>110.334</b></td>
                    <td>102.347</td>
                    <td>88.674</td>
                    <td>81.615</td>
                    <td>67.634</td>
                    <td>61.940</td>
                    <td>53.926</td>
                </tr>
                <tr>
                    <td>DB</td>
                    <td>1.419</td>
                    <td><b>2.161</b></td>
                    <td>1.568</td>
                    <td>1.594</td>
                    <td>1.548</td>
                    <td>1.737</td>
                    <td>1.712</td>
                </tr>
                <tr>
                    <td>DB*</td>
                    <td>1.749</td>
                    <td><b>2.289</b></td>
                    <td>1.736</td>
                    <td>1.830</td>
                    <td>1.893</td>
                    <td>1.849</td>
                    <td>2.133</td>
                </tr>
                <tr>
                    <td>D</td>
                    <td>0.057</td>
                    <td><b>0.051</b></td>
                    <td>0.083</td>
                    <td>0.080</td>
                    <td>0.098</td>
                    <td>0.075</td>
                    <td>0.084</td>
                </tr>
                <tr>
                    <td>COP</td>
                    <td><b>0.179</b></td>
                    <td>0.159</td>
                    <td>0.138</td>
                    <td>0.133</td>
                    <td>0.126</td>
                    <td>0.120</td>
                    <td>0.124</td>
                </tr>
                </tbody>
            </table>

            <p>Most parameters including the silhouette coefficient suggests that the best choice of <b>k</b> is 5. The
                clustering result of <b>k=5</b> is
                shown below. The clustering result is similar to the KMeans in Python.</p>

            <img style="width:95%" class="centerfig" src="picture/cluster/r_2y_kmeans5.png">
            <figurecaption>Figure: visualization of clusters in KMeans model with k=5</figurecaption>

            <p>With k=5, cluster 1 is GPU-dominated that follow the abnormal increasing trend in prices. Cluster 2,3 are CPU-Drive mixed cluster, and cluster 4,5 are drive-dominated cluster that follow the normal decreasing trend in prices.</p>

            <table style="text-align:center;width:100%">
                <caption>Type of products in each cluster of KMeans(5) model in R</caption>
                <col width=25%>
                <col width=25%>
                <col width=25%>
                <col width=25%>
                <tr>
                    <th style="text-align:center"></th>
                    <th style="text-align:center">CPU</th>
                    <th style="text-align:center">GPU</th>
                    <th style="text-align:center">Drive</th>
                </tr>
                <tr>
                    <td>Cluster 1</td>
                    <td>7</td>
                    <td><b>52</b></td>
                    <td>7</td>
                </tr>
                <tr>
                    <td>Cluster 2</td>
                    <td><b>16</b></td>
                    <td>4</td>
                    <td><b>26</b></td>
                </tr>
                <tr>
                    <td>Cluster 3</td>
                    <td><b>11</b></td>
                    <td></td>
                    <td><b>19</b></td>
                </tr>
                <tr>
                    <td>Cluster 4</td>
                    <td>4</td>
                    <td></td>
                    <td><b>19</b></td>
                </tr>
                <tr>
                    <td>Cluster 5</td>
                    <td>5</td>
                    <td>3</td>
                    <td><b>35</b></td>
                </tr>
            </table>

            <p>Then, a hierarchical clustering model is trained using DTW metrics. The dendrogram is shown below. Also,
                a <b>HUGE</b> picture of all time series can be found at <a href="picture/cluster/amazon_ts.png">picture/cluster/amazon_ts.png</a>.
            </p>

            <img style="width:95%" class="centerfig" src="picture/cluster/amazon_dend.png">
            <figurecaption>Figure: dendrogram of hierarchical model with k=5 (color bar: red-gpu, gold-cpu,
                blue-drive)
            </figurecaption>

            <p>The color bar in the dendrogram is the product type, where CPUs are golden, GPUs are red, and drives are blue). With k=5, three clusters on the left (pink, yellow, and green) are mainly drives and CPUs which follows the normal decreasing trend. Two clusters on the right (blue and purple) are mainly GPUs which follows the abnormal increasing trend. These clusters are consistent with the color bar. So, a good choice of k is 5.</p>

            <p>To display the similarity between these products, three heatmaps are created using euclidean distance, DTW distance, and cosine similarity.</p>

            <img style="width:32%" src="picture/cluster/r_dist_eu.png">
            <img style="width:32%" src="picture/cluster/r_dist_dtw.png">
            <img style="width:32%" src="picture/cluster/r_dist_cos.png">
            <figurecaption>Figure: heatmaps of amazon products using euclidean distance, DTW distance, and cosine similarity
            </figurecaption>

            <p>The heatmaps show that CPU and drives have similar price time series, and GPUs have quite different price trend compare to CPUs and drives.</p>

        </section>

        <section id="cc-pred">

            <h3>2.3 Cluster Prediction of Crypto Time Series</h3>

            <p>KMeans model with k=5 trained in Part 2.2 is used to predict all 8 crypto time series. The prediction is
                shown below. All cryptos are assigned to cluster 1 which is the graphic card cluster. This result
                indicates a strong similarity between graphic cards' prices and cryptocurrencies' prices and an
                auto-similarity among cryptocurrencies' prices themselves.</p>

            <table style="text-align:center;width:100%">
                <caption>Predicted clusters of cryptocurrencies using KMeans model with k=5</caption>
                <col width=12.5%>
                <col width=12.5%>
                <col width=12.5%>
                <col width=12.5%>
                <col width=12.5%>
                <col width=12.5%>
                <col width=12.5%>
                <col width=12.5%>
                <thead>
                <tr>
                    <th style="text-align:center">BTC</th>
                    <th style="text-align:center">ETH</th>
                    <th style="text-align:center">DOGE</th>
                    <th style="text-align:center">LTC</th>
                    <th style="text-align:center">BCH</th>
                    <th style="text-align:center">ETC</th>
                    <th style="text-align:center">XMR</th>
                    <th style="text-align:center">DASH</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                    <td>1</td>
                </tr>
                </tbody>
            </table>

            <p>A visualization of these time series is shown below.</p>

            <img style="width:60%" class="centerfig" src="picture/cluster/crypto.png">
            <figurecaption>Figure: visualization of 8 cryptocurrencies time series(color as their clusters)
            </figurecaption>

            <h3>2.4 Cluster Prediction of Commodity Time Series</h3>

            <p>KMeans model with k=5 trained in Part 2.2 is used to predict all 5 commodities time series. The
                prediction is shown below. </p>

            <table style="text-align:center;width:100%">
                <caption>Predicted clusters of commodities using KMeans model with k=5</caption>
                <col width=20%>
                <col width=20%>
                <col width=20%>
                <col width=20%>
                <col width=20%>
                <thead>
                <tr>
                    <th style="text-align:center">Aluminum</th>
                    <th style="text-align:center">Copper</th>
                    <th style="text-align:center">Gold</th>
                    <th style="text-align:center">Silver</th>
                    <th style="text-align:center">Crude Oil</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>5</td>
                    <td>2</td>
                    <td>4</td>
                    <td>2</td>
                    <td>5</td>
                </tr>
                </tbody>
            </table>

            <p>A visualization of these time series is shown below.</p>

            <img style="width:60%" class="centerfig" src="picture/cluster/commodity.png">
            <figurecaption>Figure: visualization of 5 commodities time series(color as their clusters)</figurecaption>

        </section>

        <h3>2.5 Summary of Time Series Clustering by R</h3>

        <section id="cc-sum">

            <p>In this part, KMeans clustering and hierarchical clustering models are applied to the 2-year Amazon
                historical price data using DTW metrics. The best k=5 of KMeans model is selected by multiple validation
                parameters including silhouette coefficient. The clustering result also suggests that graphic cards have
                taken the biggest impact from the pandemic and cryptocurrency boom.</p>

            <p>Cryptocurrencies and commodities are used as new data to the KMeans model. The predicted clusters
                indicate that there are strong correlation between the price of graphic cards and the price of
                cryptocurrencies.</p>

        </section>

        <h2>3. YouTube Comments (Python Text Data)</h2>

        <p>The Python script for this part is <a
                href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/blob/master/cluster/youtube.ipynb">cluster/youtube.ipynb</a>.
        </p>

        <h3>3.1 Preparation of YouTube Comments</h3>

        <section id="youtube-pre">

            <p>First, all comments labeled as 0(which means not assigned to any topic) or -1(which means assigned to
                multiple topics) are removed in this part. Also, since comments with very little words are likely to be
                meaningless and comments with too many words are likely to talk about multiple topics. The word counts
                in comments are limited in the range from <b>5 to 40</b>. As a result, only <b>18,400 records</b> are
                selected from the <a
                        href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/blob/master/data/comments_labeled.csv">ata/comments_labeled.csv</a>
                with over 140,000 comments.</p>

            <p>After cleaning, <b>CountVectorizer</b> and <b>TfidfVectorizer</b> with <b>max_feature=500</b> are applied
                to the selected comments to create the document-term matrix. Since the dataset is large enough, each DTM
                contains a full 500 features as expected. A PCA decomposition model with n_components=2 is applied to
                the comments to create 2D data for plotting.</p>

        </section>

        <h3>3.2 KMeans Clustering on YouTube comments (metric: Euclidean)</h3>

        <section id="youtube-km">

            <p>KMeans models with k=2:7 are trained by the DTM from CountVectorizer and TfidfVectorizer. The clustering
                results of k=2,4,6 are shown below.</p>

            <img style="width:47%" src="picture/cluster/kmeans_cv_2.png">
            <img style="width:48%" src="picture/cluster/kmeans_tfv_2.png">
            <img style="width:47%" src="picture/cluster/kmeans_cv_4.png">
            <img style="width:48%" src="picture/cluster/kmeans_tfv_4.png">
            <img style="width:47%" src="picture/cluster/kmeans_cv_6.png">
            <img style="width:48%" src="picture/cluster/kmeans_tfv_6.png">

            <figurecaption>Figure: visualization of clusters in KMeans model with k=2,4,6 using CountVectorizer and
                TfidfVectorizer
            </figurecaption>

            <p>Also, confusion matrix are calculated using the topic labels. The confusion matrix of KMeans model with
                k=2,4,6 using CountVectorizer are shown below.</p>

            <table style="text-align:center;width:100%">
                <caption>Confusion matrix of KMeans model with k=2</caption>
                <thead>
                <tr>
                    <th></th>
                    <th style="text-align:center">Topic 1</th>
                    <th style="text-align:center">Topic 2</th>
                    <th style="text-align:center">Topic 3</th>
                    <th style="text-align:center">Topic 4</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>Cluster 0</td>
                    <td>751</td>
                    <td>1859</td>
                    <td>22</td>
                    <td>15</td>
                </tr>
                <tr>
                    <td>Cluster 1</td>
                    <td>5702</td>
                    <td>3849</td>
                    <td>3152</td>
                    <td>3139</td>
                </tr>
                </tbody>
            </table>

            <table style="text-align:center;width:100%">
                <caption>Confusion matrix of KMeans model with k=4</caption>
                <thead>
                <tr>
                    <th></th>
                    <th style="text-align:center">Topic 1</th>
                    <th style="text-align:center">Topic 2</th>
                    <th style="text-align:center">Topic 3</th>
                    <th style="text-align:center">Topic 4</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>Cluster 0</td>
                    <td>2751</td>
                    <td>2</td>
                    <td>3</td>
                    <td>2</td>
                </tr>
                <tr>
                    <td>Cluster 1</td>
                    <td>1727</td>
                    <td>2959</td>
                    <td>3170</td>
                    <td>3150</td>
                </tr>
                <tr>
                    <td>Cluster 2</td>
                    <td>11</td>
                    <td>2747</td>
                    <td>1</td>
                    <td>2</td>
                </tr>
                <tr>
                    <td>Cluster 3</td>
                    <td>1964</td>
                    <td>0</td>
                    <td>0</td>
                    <td>0</td>
                </tr>
                </tbody>
            </table>

            <table style="text-align:center;width:100%">
                <caption>Confusion matrix of KMeans model with k=6</caption>
                <thead>
                <tr>
                    <th></th>
                    <th style="text-align:center">Topic 1</th>
                    <th style="text-align:center">Topic 2</th>
                    <th style="text-align:center">Topic 3</th>
                    <th style="text-align:center">Topic 4</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>Cluster 0</td>
                    <td>0</td>
                    <td>0</td>
                    <td>1777</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>Cluster 1</td>
                    <td>9</td>
                    <td>2736</td>
                    <td>1</td>
                    <td>2</td>
                </tr>
                <tr>
                    <td>Cluster 2</td>
                    <td>1978</td>
                    <td>0</td>
                    <td>0</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>Cluster 3</td>
                    <td>2266</td>
                    <td>0</td>
                    <td>0</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td>Cluster 4</td>
                    <td>2200</td>
                    <td>2972</td>
                    <td>1396</td>
                    <td>1914</td>
                </tr>
                <tr>
                    <td>Cluster 5</td>
                    <td>0</td>
                    <td>0</td>
                    <td>0</td>
                    <td>1238</td>
                </tr>
                </tbody>
            </table>

            <p>From the confusion matrix, it's obvious that KMeans with k=2 doesn't separate the topic well. When k=4, there are two clusters that mainly contain Topic 1(GPU) comments and a cluster that mainly contains Topic 2(CPU) comments. When k increases to 6, more mono-topic clusters appear, for example, cluster 0 is a Topic 3(Phone) cluster and cluster 5 is a Topic 4(Tech Company) cluster. </p>

            <p>More than one Topic 1(GPU) cluster appear in these models indicates that this topic can be separated to some sub-topics. Wordclouds of two Topic 1 clusters in KMeans(6) is shown below.</p>

            <img style="width:47%" src="picture/cluster/kmeans_wc_3.png">
            <img style="width:48%" src="picture/cluster/kmeans_wc_5.png">
            <figurecaption>Figure: wordclouds of Topic 1 clusters in KMeans model with k=6 using CountVectorizer
            </figurecaption>

            <p>To determine the best k of KMeans model, a silhouette analysis is performed. The KMeans(6) model gives the largest silhouette coefficient 0.12. The results are shown below.</p>

            <img style="width:90%" class="centerfig" src="picture/cluster/sil_cv_3.png">
            <img style="width:90%" class="centerfig" src="picture/cluster/sil_cv_4.png">
            <img style="width:90%" class="centerfig" src="picture/cluster/sil_cv_5.png">
            <img style="width:90%" class="centerfig" src="picture/cluster/sil_cv_6.png">
            <figurecaption>Figure: silhouette analysis of KMeans model with k=3~6 using CountVectorizer
            </figurecaption>

        </section>

        <h3>3.3 DBScan & Hierarchical Clustering on YouTube comments (metric: Cosine)</h3>

        <section id="youtube-other">

            <p>A DBScan model is trained by the DTM from CountVectorizer. However, after multiple attempts on <b>eps</b>, <b>min_sample</b>, the model fails to give a reasonable clustering result. The result from a DBScan model with eps=0.13 and min_sample=20 is shown below.</p>

            <table style="text-align:center;width:100%">
                <caption>Confusion matrix of DBScan model with eps=0.13 and min_sample=20</caption>
                <thead>
                <tr>
                    <th></th>
                    <th style="text-align:center">Topic 1</th>
                    <th style="text-align:center">Topic 2</th>
                    <th style="text-align:center">Topic 3</th>
                    <th style="text-align:center">Topic 4</th>
                </tr></thead><tbody>
 <tr><td>Cluster 0</td><td>6299</td><td>4448</td><td>3163</td><td>3152</td></tr>
 <tr><td>Cluster 1</td><td>6</td><td>921</td><td>1</td><td>1</td></tr>
 <tr><td>Cluster 2</td><td>0</td><td>41</td><td>0</td><td>0</td></tr>
 <tr><td>Cluster 3</td><td>0</td><td>94</td><td>0</td><td>0</td></tr>
 <tr><td>Cluster 4</td><td>0</td><td>87</td><td>0</td><td>0</td></tr>
 <tr><td>Cluster 5</td><td>20</td><td>0</td><td>0</td><td>0</td></tr>
 <tr><td>Outlier</td><td>128</td><td>117</td><td>10</td><td>1</td></tr>
</tbody></table>

            <p>The model only gives several small clusters and one huge cluster with mixed topics. It perhaps because the distance between text data is not well-defined. CountVectorizer does not normalize the text vector, distance between comments is also affected by the length of comments.</p>

            <p>A hierarchical model is trained by the DTM from TfidfVectorizer. The dendrogram is shown below.</p>

            <img style="width:70%" class="centerfig" src="picture/cluster/hier_cv.png">

            <figurecaption>Figure: dendrogram of hierarchical model using TfidfVectorizer
            </figurecaption>

            <p>The confusion matrix is calculated based on the hierarchical model with 8 clusters.</p>

            <table style="text-align:center;width:100%">
                <caption>Confusion matrix of hierarchical model with 8 clusters</caption>
                <thead>
                <tr>
                    <th></th>
                    <th style="text-align:center">Topic 1</th>
                    <th style="text-align:center">Topic 2</th>
                    <th style="text-align:center">Topic 3</th>
                    <th style="text-align:center">Topic 4</th>
                </tr></thead><tbody>
 <tr><td>Cluster 1</td><td>0</td><td>88</td><td>0</td><td>0</td></tr>
 <tr><td>Cluster 2</td><td>0</td><td>570</td><td>0</td><td>0</td></tr>
 <tr><td>Cluster 3</td><td>55</td><td>0</td><td>0</td><td>0</td></tr>
 <tr><td>Cluster 4</td><td>1475</td><td>1</td><td>0</td><td>0</td></tr>
 <tr><td>Cluster 5</td><td>0</td><td>0</td><td>470</td><td>25</td></tr>
 <tr><td>Cluster 6</td><td>56</td><td>2554</td><td>6</td><td>11</td></tr>
 <tr><td>Cluster 7</td><td>4497</td><td>65</td><td>18</td><td>34</td></tr>
 <tr><td>Cluster 8</td><td>370</td><td>2430</td><td>2680</td><td>3084</td></tr>
</tbody></table>

            <p>The hierarchical model has a better clustering result comparing to the DBScan model. Cluster 2,4,5,6,7 are both mono-topic clusters with hundreds and thousands comments. Cluster 1,3 are mono-topic clusters but only have small amount of comments. Cluster 8 is a mixed-topic cluster.</p>

        </section>

        <h3>3.4 Summary of Text Data Clustering</h3>

        <section id="youtube-sum">

            <p>In this part, three different clustering method is applied to the YouTube comments dataset with 4 topics. KMeans models with k=4~6 are able to give some mono-topic clusters. Also, the results suggest that more than one sub-topic can be separated from the graphic card topic. The best choice of k is 6, which is chosen by the silhouette analysis. The DBScan model fails to give a reasonable clustering result due to the high dimensionality of text data and the information loss during decomposition. The hierarchical model is able to give some mono-topic clusters like the KMeans model. </p>

            <p>Most models give a huge mixed-topic cluster. This remaining mixed-topic cluster indicates some implicit similarities between comments that are failed to cover by the pre-label (remember the comments are collected from the same YouTube channel, maybe many comments contain the channel name or the host name).</p>

        </section>

    </div>

</div>

<script src="jquery-3.5.1.min.js"></script>
<script>$(function () {
    $(".headerpage-501").load("header.html");
});</script>
<script src="nav_scroll.js"></script>
</body>
</html>