<!DOCTYPE HTML>
<html>
<head>
    <title>Jingsong Gao - 501 Portfolio</title>
    <meta charset="utf-8">
    <link rel="stylesheet" href="style.css"/>
    <link rel="shortcut icon" href="/icon.png"/>
    <link rel="bookmark" href="/icon.png"/>
</head>

<body>
<div class="headerpage-501"></div>
<div class="main">
    <div class="left-side">
        <div class="left-navi">
            <ul>
                <li class="nav"><p>Links</p></li>
                <li class="nav"><a href="#pre">Scripts</a></li>
                <li class="nav"><a href="#data">Data</a></li>
                <li class="nav"><p>YouTube Comments</p></li>
                <li class="nav"><a href="#yt-pur">Purpose</a></li>
                <li class="nav"><a href="#yt-word">Tokenizer: Word</a></li>
                <li class="nav"><a href="#yt-itemset">Tokenizer: Item Set</a></li>
                <li class="nav"><a href="#yt-rf">Random Forest</a></li>
                <li class="nav"><a href="#yt-summary">Summary</a></li>
                <li class="nav"><p>Time Series</p></li>
                <li class="nav"><a href="#ts-pur">Purpose</a></li>
                <li class="nav"><a href="#ts-feat">Feature Extraction</a></li>
                <li class="nav"><a href="#ts-class">Classification</a></li>
                <li class="nav"><a href="#ts-summary">Summary</a></li>
            </ul>
        </div>
    </div>

    <div class="content">
        <h1 style="text-align:center">Decision Tree</h1>

        <h2>1. Links</h2>

        <h3>1.1 Scripts</h3>
            <section id="pre">
                <p>YouTube Comments Decision Tree: <a
                        href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/tree/master/decision_tree/text.ipynb"
                        target="_blank">text.ipynb</a>, <a
                        href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/tree/master/decision_tree/text_new.ipynb"
                        target="_blank">text_new.ipynb</a>.</p>
                <p>Amazon Price Decision Tree: <a href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/tree/master/decision_tree/record.Rmd" target="_blank">record.Rmd</a>.</p>

            </section>

        <h3>1.2 Data</h3>
            <section id="data">
                <p>YouTube Comments dtm by ARM: <a href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/tree/master/decision_tree/comments_arm.binary"
                        target="_blank">comments_arm.binary</a>
                <figure style="width:90%">
                    <img class="center-fig" src="picture/decision_tree/text_overview.png">
                    <figurecaption>Screenshot: frequent-itemset-based DTM data converted from YouTube Comments
                    </figurecaption>
                </figure>
                <figure style="width:90%">
                    <img class="center-fig" src="picture/decision_tree/wordcloud.png">
                    <figurecaption>Figure: "wordcould" of frequent-itemset-based DTM data</figurecaption>
                </figure>

                <p>YouTube Comments labeled by Decision Tree: <a
                        href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/tree/master/decision_tree/comments_dt_filled.csv"
                        target="_blank">comments_dt_filled.csv</a>.</p>
                <p>Amazon Price Dats: <a href="https://github.com/jingshenSN2/GU-ANLY-501-FALL-2021/tree/master/decision_tree/ts.csv" target="_blank">ts.csv</a>.</p>

            </section>

        <h2>2. YouTube Comments Classification</h2>

        <h3>2.1 Purpose of Applying Decision Tree on Labeled Comments</h3>
            <section id="yt-pur">
                <p>In previous part, the labels of YouTube comments which were automatically generated by keyword matching. And in ARM part, some labels are proved to be inaccurate. After some tweaks on the selection of keywords  and categories, now there are 5 different labels that represent different topics including: GPU, CPU, Phone, Tech Company, and Channel. In this part, these labels will be reexamined by decision tree classifier.</p>

                <p>To take advantage of ARM results, the YouTube comments are also re-tokenized by frequent item sets with length=2. The re-tokenized DTM(document term matrix) describes the occurrence of item sets instead of single word in every comment. </p>

            </section>

        <h3>2.2 Decision Trees built by Words Tokenizer</h3>
            <section id="yt-word">
                <p>Two decision trees using entropy and gini as criterion is trained by word-tokenized (CountVectorizer) data. Other parameters are max_depth=15, min_samples_split=10, min_samples_leaf=10. The visualization of the trees are shown below.</p>
                <figure style="width:90%">
                    <img style="width:49%" src="picture/decision_tree/cls_e.png">
                    <img style="width:49%" src="picture/decision_tree/cls_g.png">
                    <figurecaption>Figure: decision tree without class-weight param, criterion: left-entropy(acc=0.8900), right-gini(acc=0.8922)
                    </figurecaption>
                </figure>
                <p>The confusion matrices are shown below.</p>
                <table class="center-table">
                <caption>Confusion matrix of decision tree using Entropy criterion and word tokenizer</caption>
<thead><tr><th></th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead><tbody>
 <tr><td>1:GPU</td><td>8575</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
 <tr><td>2:CPU</td><td>1514</td><td>3865</td><td>0</td><td>0</td><td>0</td></tr>
 <tr><td>3:Phone</td><td>1559</td><td>0</td><td>3206</td><td>0</td><td>0</td></tr>
 <tr><td>4:TechCompany</td><td>408</td><td>0</td><td>0</td><td>3990</td><td>0</td></tr>
 <tr><td>5:Channel</td><td>1429</td><td>0</td><td>0</td><td>0</td><td>20102</td></tr>
</tbody></table>

                <table class="center-table">
                <caption>Confusion matrix of decision tree using GINI criterion and word tokenizer</caption>
<thead><tr><th></th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead><tbody> <tr><td>1:GPU</td><td>8575</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
 <tr><td>2:CPU</td><td>999</td><td>4380</td><td>0</td><td>0</td><td>0</td></tr>
 <tr><td>3:Phone</td><td>1559</td><td>0</td><td>3206</td><td>0</td><td>0</td></tr>
 <tr><td>4:TechCompany</td><td>828</td><td>0</td><td>0</td><td>3570</td><td>0</td></tr>
 <tr><td>5:Channel</td><td>1429</td><td>0</td><td>0</td><td>0</td><td>20102</td></tr>
</tbody></table>

                <p>The accuracy of two trees are very close(0.8900 vs 0.8922). Also, the root nodes of them are both 'intel<0.5' that separate some comments of the CPU topic. The shape of these trees are quite 'straight', instead of a 'decision tree', it's more like a 'decision linked list'. This result does make sense, since the topics are labeled by some keywords and keywords are used to classify them again in this tree.</p>

                <p>However, the confusion matrices of them indicate the models are under-fitted, which can be also seen from the visualization, that the last node can be more separated. Therefore, maybe a larger max_depth will lead to a better accuracy and confusion matrix.</p>

            </section>

        <h3>2.3 Decision Tree built by Item Sets Tokenizer</h3>
            <section id="yt-itemset">
                <p>In this part, one decision tree using gini as criterion is trained by itemset-tokenized data. Other parameters are max_depth=35, min_samples_split=10. The visualization of the trees are shown below.</p>
                <figure style="width:90%">
                    <img class="center-fig" src="picture/decision_tree/cls_arm.png">
                    <figurecaption>Figure: decision tree using frequent-itemset-based DTM data(acc=0.5884)</figurecaption>
                </figure>

                <p>The confusion matrix is shown below.</p>
                <table class="center-table">
                <caption>Confusion matrix of decision tree using GINI criterion and item set tokenizer</caption>
<thead><tr><th></th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead><tbody>  <tr><td>1:GPU</td><td>2200</td><td>0</td><td>0</td><td>0</td><td>6375</td></tr>
 <tr><td>2:CPU</td><td>0</td><td>1620</td><td>0</td><td>0</td><td>3759</td></tr>
 <tr><td>3:Phone</td><td>0</td><td>0</td><td>600</td><td>0</td><td>4165</td></tr>
 <tr><td>4:TechCompany</td><td>0</td><td>0</td><td>0</td><td>321</td><td>4077</td></tr>
 <tr><td>5:Channel</td><td>0</td><td>0</td><td>0</td><td>0</td><td>21531</td></tr>
</tbody></table>
                <p>This decision tree has a relatively low accuracy of 0.5884, even if it's depth is more than twice as the trees built by word-tokenized data. Also, a similar "linked list" shape appears in this tree, which means most classes can be labeled by a single itemset, for example, {amd, intel} -> CPU topic, {buy, card} -> GPU topic. </p>

            </section>

        <h3>2.4 Random Forest built by Item Sets Tokenizer</h3>
            <section id="yt-rf">
                <p>A random forest contains 100 decision trees using gini criterion with max_dept=35 is trained by itemset-tokenized data. Other parameters are max_features=50, max_samples=1000, min_samples_split=10. The confusion matrix is shown below.</p>
                <table class="center-table">
                <caption>Confusion matrix of decision tree using GINI criterion and item set tokenizer</caption>
<thead><tr><th></th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th></tr></thead><tbody>   <tr><td>1:GPU</td><td>2233</td><td>0</td><td>0</td><td>0</td><td>6342</td></tr>
 <tr><td>2:CPU</td><td>0</td><td>1751</td><td>0</td><td>0</td><td>3628</td></tr>
 <tr><td>3:Phone</td><td>0</td><td>0</td><td>518</td><td>0</td><td>4247</td></tr>
 <tr><td>4:TechCompany</td><td>1</td><td>0</td><td>0</td><td>157</td><td>4240</td></tr>
 <tr><td>5:Channel</td><td>2</td><td>0</td><td>0</td><td>0</td><td>21529</td></tr>
</tbody></table>

                <p>The accuracy of this random forest is 0.5865, which is very close to a single decision tree.</p>

            </section>

        <h3>2.5 Summary of Decision Trees from YouTube Comments</h3>
            <section id="yt-summary">
                <p>In this section, first, the labels automatically assigned by keywords are examined by decision tree. Then, the frequent itemsets in ARM are used to tokenize the text data and train a decision tree and a random forest.</p>

            </section>

        <h2>3. Time Series</h2>

        <h3>3.1 Purpose of Applying Decision Tree on Time Series</h3>
            <section id="ts-pur">
                <p>As mentioned in the introduction, one of the data science questions is whether Intel would reduce their CPUs' prices when AMD release their new generations of CPU. In this part, two representative release dates(Intel 10th Gen: 2020.4.30, AMD Zen3 Gen: 2020.10.7) are selected to examine this question.</p>
            </section>

        <h3>3.2 Feature Extraction</h3>
            <section id="ts-feat">
                <p>The prices of CPU from two months before to two months after release dates are selected for classification. The prices are normalized to the closest release date, for example, March price is divided by May price, September price is divided by October price, etc. All missing prices are replaced by 1. Also, two categorical variables are calculated from the monthly prices - Zen3Down and Gen10Down, which means whether the average price after release date is lower than before. The dataset is shown below.</p>
                <figure style="width:90%">
                    <img class="center-fig" src="picture/decision_tree/timeseries.png">
                    <figurecaption>Screenshot: 12 features extracted from Amazon product price time series </figurecaption>
                </figure>
            </section>

        <h3>3.3 Classification</h3>
            <section id="ts-class">
                <p>First, only Zen3Down and monthly prices from March to July are used to build a decision tree. Other parameters are minsplit = 2, minbucket = 2, cp = 0.01, maxdepth = 4. The visualization of this tree is shown below.</p>
                <figure style="width:90%">
                    <img class="center-fig" src="picture/decision_tree/dt_ts_zen3.png">
                    <figurecaption>Figure: decision tree using features 2020.03-2020.07, Zen3Down(acc=0.8557) </figurecaption>
                </figure>

                <p>The confusion matrix is shown below.</p>
                <table class="center-table" style="width: 50%">
                <caption>Confusion matrix of decision tree using Zen3-related features</caption>
<thead><tr><th></th><th>AMD</th><th>Intel</th></thead><tbody>
 <tr><td>1:AMD</td><td>20</td><td>1</td></tr>
 <tr><td>2:Intel</td><td>13</td><td>63</td></tr>
</tbody></table>
                <p>The accuracy of this tree is 0.8557, and there are 13 AMD cpus are wrongly labeled as Intel. From the visualization, it shows that the prices of most Intel CPUs are lower after Zen3 released.</p>
                <p>Then, only Gen10Down and monthly prices from August to December are used to build a decision tree. Other parameters are minsplit = 2, minbucket = 2, cp = 0.01, maxdepth = 4. The visualization of this tree is shown below.</p>
                <figure style="width:90%">
                    <img class="center-fig" src="picture/decision_tree/dt_ts_gen10.png">
                    <figurecaption>Figure: decision tree using features 2020.08-2020.12, Gen10Down(acc=0.9175) </figurecaption>
                </figure>

                <p>The confusion matrix is shown below.</p>
                <table class="center-table" style="width: 50%">
                <caption>Confusion matrix of decision tree using Gen10-related features</caption>
<thead><tr><th></th><th>AMD</th><th>Intel</th></thead><tbody>
 <tr><td>1:AMD</td><td>27</td><td>2</td></tr>
 <tr><td>2:Intel</td><td>6</td><td>62</td></tr>
</tbody></table>
                <p>The accuracy of this tree is 0.9175, and there are 6 AMD cpus are wrongly labeled as Intel, and 2 vice versa. The result is better than using Zen3 related features. However, from the visualization, it shows that the prices of most AMD CPUs are actually higher after Intel 10th Gen released.</p>
                <p>At last, all indicators and monthly prices are used to build a decision tree. Other parameters are still minsplit = 2, minbucket = 2, cp = 0.01, maxdepth = 4. The visualization of this tree is shown below.</p>
                <figure style="width:90%">
                    <img class="center-fig" src="picture/decision_tree/dt_ts_all.png">
                    <figurecaption>Figure: decision tree using all features(acc=0.9485) </figurecaption>
                </figure>

                <p>The confusion matrix is shown below.</p>
                <table class="center-table" style="width: 50%">
                <caption>Confusion matrix of decision tree using all features</caption>
<thead><tr><th></th><th>AMD</th><th>Intel</th></thead><tbody>
 <tr><td>1:AMD</td><td>29</td><td>1</td></tr>
 <tr><td>2:Intel</td><td>4</td><td>63</td></tr>
</tbody></table>
                <p>The accuracy of this tree is 0.9485, and there are only 5 mis-classifications in total. The result is better than only using part of features. From the visualization, it also shows that the prices of most AMD CPUs are actually higher after Intel 10th Gen released, for example, the split of root node shows that if prices in December is higher than October, it's more likely to be an AMD CPU.</p>
            </section>

        <h3>3.4 Summary of Decision Trees in Time Series</h3>
            <section id="ts-summary">
                <p></p>
            </section>
    </div>

</div>

<script src="jquery-3.5.1.min.js"></script>
<script>$(function () {
    $(".headerpage-501").load("header.html");
});</script>
<script src="nav_scroll.js"></script>
</body>
</html>